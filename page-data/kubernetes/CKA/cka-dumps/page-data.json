{"componentChunkName":"component---src-templates-blog-post-js","path":"/kubernetes/CKA/cka-dumps/","result":{"data":{"markdownRemark":{"frontmatter":{"title":"CKA 시험 준비 기출문제","date":"November 02, 2024"},"html":"<h1>CKA 기출문제 회독</h1>\n<p>CKA는 기출문제가 많이 공개되고, 또 재출제율이 매우 높은 시험으로 알고 있다. 그래서 이번 기회에는 한번 기출문제들을 한번 씩 읽어보고 받아 쳐보고 공부해보려 한다.</p>\n<h2>Exam Topics 기출문제</h2>\n<p><a href=\"https://www.examtopics.com/exams/cncf/cka/view/\">https://www.examtopics.com/exams/cncf/cka/view/</a></p>\n<p><strong>Question #1</strong></p>\n<pre><code>Context -  \nYou have been asked to create a new ClusterRole for a deployment pipeline and bind it to a specific ServiceAccount scoped to a specific namespace.\n\nTask -   \nCreate a new ClusterRole named deployment-clusterrole, which only allows to create the following resource types:\n✑ Deployment\n✑ Stateful Set\n✑ DaemonSet\n\nCreate a new ServiceAccount named cicd-token in the existing namespace app-team1.\n\nBind the new ClusterRole deployment-clusterrole to the new ServiceAccount cicd-token, limited to the namespace app-team1.\n</code></pre>\n<p><code>kubectl config use-context k8s</code></p>\n<p><code>kubectl create clusterrole deployment-clusterrole --verb=create --resource=Deployment,StatefulSet,DaemonSet</code></p>\n<p><code>kubectl create sa cicd-token -n app-team1</code></p>\n<p><code>kubectl create clusterrolebinding deploy-b --clusterrole=deployment-clusterrole --serviceaccount=app-team1:cicd-token</code></p>\n<p>(오늘 공교롭게도 스터디에서 나온 주제다. 클러스터롤, 클러스터 롤 바인딩, 그리고 SA)</p>\n<p><strong>Question #2</strong><br>\nTask -<br>\nSet the node named ek8s-node-0 as unavailable and reschedule all the pods running on it.</p>\n<p><code>kubectl config use-context ek8s</code></p>\n<p><code>kubectl get nodes</code></p>\n<p><code>kubectl drain ek8s-node-0 --ignore-daemonsets</code></p>\n<ul>\n<li>이때 사용되는 <code>drain</code> 명령어는 해당 노드를 비활성화하고, 해당 노드에 있는 모든 파드를 다른 노드로 이동시킨다. <code>--ignore-daemonsets</code> 옵션을 사용하면 데몬셋을 무시하고 파드를 이동시킨다.</li>\n</ul>\n<p><code>kubectl drain ek8s-node-0 --ignore-daemonsets --delete-emptydir-data</code></p>\n<ul>\n<li><code>--delete-emptydir-data</code> 옵션을 사용하면 노드를 비활성화하기 전에 해당 노드의 emptyDir 볼륨을 삭제한다.</li>\n</ul>\n<p><code>kubectl get nodes</code></p>\n<p>(문제에서는 ek8s-node-0를 비활성화하라고 했지만, 사진에서는 왜 ek8s-node-1을 비활성화했는지 모르겠다.)</p>\n<p><strong>Question #3</strong><br>\nTask -<br>\nGiven an existing Kubernetes cluster running version 1.22.1, upgrade all of the Kubernetes control plane and node components on the master node only to version 1.22.2.<br>\nBe sure to drain the master node before upgrading it and uncordon it after the upgrade.</p>\n<p><code>kubectl config use-context mk8s</code></p>\n<p><code>kubectl get nodes</code></p>\n<p><code>kubectl drain mk8s-master-0 --ignore-daemonsets</code></p>\n<ul>\n<li>cordoned 뜻은 노드에 파드가 스케줄링 되지 않도록 하는 것이고, drain은 노드에 있는 파드를 다른 노드로 이동시키는 것이다.</li>\n</ul>\n<p><code>ssh mk8s-master-0</code></p>\n<p><code>sudo -i</code></p>\n<ul>\n<li>이때 -i 옵션의 뜻은 root로 로그인하라는 뜻이다.</li>\n</ul>\n<p><code>apt install kubeadm=1.22.2-00 kubelet=1.22.2-00</code></p>\n<p><code>kubeadm upgrade apply v1.22.2</code></p>\n<p><code>systemctl restart kubelet</code></p>\n<p><code>exit</code></p>\n<ul>\n<li>root에서 나가기</li>\n</ul>\n<p><code>exit</code></p>\n<ul>\n<li>ssh에서 나가기</li>\n</ul>\n<p><code>kubectl uncordon mk8s-master-0</code></p>\n<ul>\n<li>uncordon은 노드에 파드가 다시 스케줄링 되도록 하는 것이다.</li>\n</ul>\n<p><code>kubectl get nodes</code></p>\n<ul>\n<li>마스터 노드가 다시 정상적으로 작동하는지 확인한다.</li>\n</ul>\n<ol start=\"4\">\n<li>Task -</li>\n</ol>\n<p>First, create a snapshot of the existing etcd instance running at <a href=\"https://127.0.0.1:2379\">https://127.0.0.1:2379</a>, saving the snapshot to /var/lib/backup/etcd-snapshot.db.</p>\n<p>The following TLS certificates/key are supplied for connecting to the server with etcdctl :\n• CA certificate: /opt/KUIN00601/ca.crt\n• Client certificate:\n/opt/KUIN00601/etcd-client.crt\n• Client key:\n/opt/KUIN00601/etcd-client.key</p>\n<p>Creating a snapshot of the given instance is expected to complete in seconds.\nIf the operation seems to hang, something's likely wrong with your command. Use CTRL + c to cancel the operation and try again.</p>\n<p>Next, restore an existing, previous snapshot located at /var/lib/backup/etcd-snapshot-previous.db.</p>\n<pre><code class=\"language-bash\">ETCDCTL_API=3 etcdctl snapshot save /var/lib/backup/etcd-snapshot.db \\\n--endpoints=https://127.0.0.1:2379 \\\n--cacert=/opt/KUIN00601/ca.crt \\\n--cert=/opt/KUIN00601/etcd-client.crt \\\n--key=/opt/KUIN00601/etcd-client.key\n</code></pre>\n<p><code>ETCDCTL_API=3 etcdctl snapshot restore /var/lib/backup/etcd-snapshot-previous.db --data-dir /var/lib/etcd-restored</code></p>\n<ol start=\"5\">\n<li>Task -</li>\n</ol>\n<p>Create a new NetworkPolicy named allow-port-from-namespace in the existing namespace fubar.\nEnsure that the new NetworkPolicy allows Pods in namespace internal to connect to port 9000 of Pods in namespace fubar.\nFurther ensure that the new NetworkPolicy:\n✑ does not allow access to Pods, which don't listen on port 9000\n✑ does not allow access from Pods, which are not in namespace internal</p>\n<pre><code class=\"language-yaml\">apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-port-from-namespace\n  namespace: fubar\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: internal\n    ports:\n    - protocol: TCP\n      port: 9000\n</code></pre>\n<ol start=\"6\">\n<li>Task -</li>\n</ol>\n<p>Reconfigure the existing deployment front-end and add a port specification named http exposing port 80/tcp of the existing container nginx.\nCreate a new service named front-end-svc exposing the container port http.\nConfigure the new service to also expose the individual Pods via a NodePort on the nodes on which they are scheduled.</p>\n<ul>\n<li>add a port specification named http exposing port 80/tcp of the existing container nginx.</li>\n</ul>\n<pre><code class=\"language-yaml\">spec:\n  containers:\n  - name: nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n      name: http\n</code></pre>\n<ul>\n<li>create a new service named front-end-svc exposing the container port http.</li>\n</ul>\n<pre><code class=\"language-bash\">kubectl expose deployment front-end --name=front-end-svc --port=80 --target-port=http --type=NodePort\n</code></pre>\n<ol start=\"7\">\n<li>Task -</li>\n</ol>\n<p>Scale the deployment presentation to 3 pods.</p>\n<p><code>kubectl scale deployment presentation --replicas=3</code></p>\n<ol start=\"8\">\n<li>Task -</li>\n</ol>\n<p>Schedule a pod as follows:\n✑ Name: nginx-kusc00401\n✑ Image: nginx\n✑ Node selector: disk=ssd</p>\n<pre><code class=\"language-yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-kusc00401\nspec:\n  nodeSelector:\n    disk: ssd\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre>\n<ol start=\"9\">\n<li>Task -</li>\n</ol>\n<p>Check to see how many nodes are ready (not including nodes tainted NoSchedule) and write the number to /opt/KUSC00402/kusc00402.txt.</p>\n<p><code>kubectl get nodes -o jsonpath='{.items[?(@.spec.taints[?(@.effect==\"NoSchedule\") == null])].status.conditions[?(@.type==\"Ready\")].status}' | grep True | wc -l > /opt/KUSC00402/kusc00402.txt</code></p>\n<p>or easy way:</p>\n<p><code>k get nodes</code><br>\n<code>echo '2' > /opt/KUSC00402/kusc00402.txt</code></p>\n<ol start=\"10\">\n<li>Task -</li>\n</ol>\n<p>Schedule a Pod as follows:\n✑ Name: kucc8\n✑ App Containers: 2\n✑ Container Name/Images:</p>\n<ul>\n<li>nginx</li>\n<li>consul</li>\n</ul>\n<pre><code class=\"language-yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: kucc8\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n  - name: consul\n    image: consul\n</code></pre>\n<p><code>kubectl apply -f kucc8-pod.yaml</code></p>\n<ol start=\"11\">\n<li>Task -</li>\n</ol>\n<p>Create a persistent volume with name app-data, of capacity 2Gi and access mode ReadOnlyMany. The type of volume is hostPath and its location is /srv/app- data.</p>\n<pre><code class=\"language-yaml\">apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: app-data\nspec:\n  capacity:\n    storage: 2Gi\n  accessModes:\n    - ReadOnlyMany\n  hostPath:\n    path: /srv/app-data\n</code></pre>\n<ol start=\"12\">\n<li>Task -</li>\n</ol>\n<p>Monitor the logs of pod foo and:\n✑ Extract log lines corresponding to error file-not-found\n✑ Write them to /opt/KUTR00101/foo</p>\n<p><code>kubectl logs foo | grep \"file-not-found\" > /opt/KUTR00101/foo</code></p>\n<ol start=\"13\">\n<li>Context -</li>\n</ol>\n<p>An existing Pod needs to be integrated into the Kubernetes built-in logging architecture (e.g. kubectl logs). Adding a streaming sidecar container is a good and common way to accomplish this requirement.</p>\n<p>Task -\nAdd a sidecar container named sidecar, using the busybox image, to the existing Pod big-corp-app. The new sidecar container has to run the following command:</p>\n<p><code>/bin/sh -c \"tail -n+1 -f /var/log/big-corp-app. log\"</code></p>\n<p>Use a Volume, mounted at /var/log, to make the log file big-corp-app.log available to the sidecar container.</p>\n<p><code>kubectl get pod big-corp-app -o yaml > big-corp-app.yaml</code></p>\n<pre><code class=\"language-yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: big-corp-app\nspec:\n  containers:\n  - name: main-container\n    image: busybox\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  - name: sidecar\n    image: busybox\n    command: [\"/bin/sh\", \"-c\", \"tail -n+1 -f /var/log/big-corp-app.log\"]\n    volumeMounts:\n    - name: varlog\n      mountPath: /var/log\n  volumes:\n  - name: varlog\n    emptyDir: {}\n</code></pre>\n<ol start=\"14\">\n<li>Task -</li>\n</ol>\n<p>From the pod label name=overloaded-cpu, find pods running high CPU workloads and write the name of the pod consuming most CPU to the file /opt/\nKUTR00401/KUTR00401.txt (which already exists).</p>\n<p><code>kubectl top pods -l name=overloaded-cpu --sort-by=cpu --no-headers | head -n 1 | awk '{print $1}' > /opt/KUTR00401/KUTR00401.txt</code></p>\n<ol start=\"15\">\n<li>Task -</li>\n</ol>\n<p>A Kubernetes worker node, named wk8s-node-0 is in state NotReady.\nInvestigate why this is the case, and perform any appropriate steps to bring the node to a Ready state, ensuring that any changes are made permanent.</p>\n<p><code>sudo systemctl restart kubelet</code><br>\n<code>sudo systemctl restart kube-proxy</code></p>\n<ol start=\"16\">\n<li>Task -</li>\n</ol>\n<p>Create a new PersistentVolumeClaim:\n✑ Name: pv-volume\n✑ Class: csi-hostpath-sc\n✑ Capacity: 10Mi\nCreate a new Pod which mounts the PersistentVolumeClaim as a volume:\n✑ Name: web-server\n✑ Image: nginx\n✑ Mount path: /usr/share/nginx/html\nConfigure the new Pod to have ReadWriteOnce access on the volume.\nFinally, using kubectl edit or kubectl patch expand the PersistentVolumeClaim to a capacity of 70Mi and record that change.</p>\n<ul>\n<li>Create a new PersistentVolumeClaim:</li>\n</ul>\n<pre><code class=\"language-yaml\">apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pv-volume\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Mi\n  storageClassName: csi-hostpath-sc\n</code></pre>\n<ul>\n<li>Create a new Pod which mounts the PersistentVolumeClaim as a volume:</li>\n</ul>\n<pre><code class=\"language-yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-server\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n      volumeMounts:\n        - mountPath: /usr/share/nginx/html\n          name: web-volume\n  volumes:\n    - name: web-volume\n      persistentVolumeClaim:\n        claimName: pv-volume\n</code></pre>\n<ul>\n<li>Expand the PersistentVolumeClaim to a capacity of 70Mi:</li>\n</ul>\n<p><code>kubectl edit pvc pv-volume</code></p>\n<ol start=\"17\">\n<li>Task -</li>\n</ol>\n<p>Create a new nginx Ingress resource as follows:\n✑ Name: pong\n✑ Namespace: ing-internal\n✑ Exposing service hello on path /hello using service port 5678</p>\n<ul>\n<li>Create a new nginx Ingress resource:</li>\n</ul>\n<pre><code class=\"language-yaml\">apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: pong\n  namespace: ing-internal\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /hello\n        pathType: Prefix\n        backend:\n          service:\n            name: hello\n            port:\n              number: 5678\n</code></pre>"}},"pageContext":{"slug":"/kubernetes/CKA/cka-dumps/"}},"staticQueryHashes":[],"slicesMap":{}}