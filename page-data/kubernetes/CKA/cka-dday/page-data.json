{"componentChunkName":"component---src-templates-blog-post-js","path":"/kubernetes/CKA/cka-dday/","result":{"data":{"markdownRemark":{"frontmatter":{"title":"CKA 시험 준비 D-day","date":"November 22, 2024"},"html":"<h1>CKA 시험 준비 D-day</h1>\n<p>망한것 같다. 일단 CKA는 주말이 아닌 이상 새벽에만 응시가 가능해서 새벽 3시에 끔찍한 악몽을 꾸고 기상했다.<br>\n시험 시작 전 환경을 확인한다고 하여 5시 30분부터 들어갔으나 생각보다 설치하고, 방을 다 보여주는데 시간이 오래 걸렸다.<br>\n6시 30분이 되어서야 시작할 수 있었으며 생각보다 엄격하게 시험 환경 검사를 한다.</p>\n<p>시험이 시작하면 PSI라는 프로그램 속에서 원격으로 다른 VM에 접속하여 시험을 볼 수 있다. 이때, 터미널에 직접 들어가서 모든 명령어를 수행하면서 시험이 진행된다.</p>\n<p>여기서 주의해야 할 점은 linux 기반 firefox 브라우저가 제공되어서 복붙이 cmd로는 되지 않으며 <code>ctrl+shift+v</code>로 복붙을 해야 하고, 복사 단축키는 결국에 못 찾았다. 또한, vim 편집기 단축어도 달랐으며, cmd+F로 검색을 할 수 없는게 시간을 많이 잡아먹었다.</p>\n<p>17문제 중에 족보랑 유사하게 나왔음에도 불구하고 시간이 부족했으며 마지막 15, 16, 17 문제는 건들지도 못했다.</p>\n<p>17 문제 중에 최소 66%를 받아야 통과이기 때문에 최소한 내가 푼 14문제 중 12문제를 맞춰야 한다는 뜻인데 어렵지 않을까 싶다.</p>\n<p>너무 아쉽지만 재응시가 한번 더 있기 때문에 다음에는 더 잘 준비해서 응시해야겠다.</p>\n<h1>출제 문제</h1>\n<ol>\n<li>Task -</li>\n</ol>\n<p>Create a new NetworkPolicy named allow-port-from-namespace in the existing namespace fubar.\nEnsure that the new NetworkPolicy allows Pods in namespace internal to connect to port 9000 of Pods in namespace fubar.\nFurther ensure that the new NetworkPolicy:\n✑ does not allow access to Pods, which don't listen on port 9000\n✑ does not allow access from Pods, which are not in namespace internal</p>\n<ol start=\"2\">\n<li>Task -</li>\n</ol>\n<p>First, create a snapshot of the existing etcd instance running at <a href=\"https://127.0.0.1:2379\">https://127.0.0.1:2379</a>, saving the snapshot to /var/lib/backup/etcd-snapshot.db.</p>\n<p>The following TLS certificates/key are supplied for connecting to the server with etcdctl :\n• CA certificate: /opt/KUIN00601/ca.crt\n• Client certificate:\n/opt/KUIN00601/etcd-client.crt\n• Client key:\n/opt/KUIN00601/etcd-client.key</p>\n<p>Creating a snapshot of the given instance is expected to complete in seconds.\nIf the operation seems to hang, something's likely wrong with your command. Use CTRL + c to cancel the operation and try again.</p>\n<p>Next, restore an existing, previous snapshot located at /var/lib/backup/etcd-snapshot-previous.db.</p>\n<ol start=\"3\">\n<li>\n<p>Task -<br>\nSet the node named ek8s-node-0 as unavailable and reschedule all the pods running on it.</p>\n</li>\n<li>\n<p>Task -</p>\n</li>\n</ol>\n<p>Reconfigure the existing deployment front-end and add a port specification named http exposing port 80/tcp of the existing container nginx.\nCreate a new service named front-end-svc exposing the container port http.\nConfigure the new service to also expose the individual Pods via a NodePort on the nodes on which they are scheduled.</p>\n<ol start=\"5\">\n<li>Task -</li>\n</ol>\n<p>Reconfigure the existing deployment front-end and add a port specification named http exposing port 80/tcp of the existing container nginx.\nCreate a new service named front-end-svc exposing the container port http.\nConfigure the new service to also expose the individual Pods via a NodePort on the nodes on which they are scheduled.</p>\n<ol start=\"6\">\n<li>Task -</li>\n</ol>\n<p>Scale the deployment presentation to 3 pods.</p>\n<ol start=\"7\">\n<li>Task -</li>\n</ol>\n<p>Schedule a pod as follows:\n✑ Name: nginx-kusc00401\n✑ Image: nginx\n✑ Node selector: disk=ssd</p>\n<ol start=\"8\">\n<li>Task -</li>\n</ol>\n<p>Check to see how many nodes are ready (not including nodes tainted NoSchedule) and write the number to /opt/KUSC00402/kusc00402.txt.</p>\n<ol start=\"9\">\n<li>Task -</li>\n</ol>\n<p>Schedule a Pod as follows:\n✑ Name: kucc8\n✑ App Containers: 2\n✑ Container Name/Images:</p>\n<ul>\n<li>nginx</li>\n<li>consul</li>\n</ul>\n<ol start=\"10\">\n<li>Task -</li>\n</ol>\n<p>Create a new nginx Ingress resource as follows:\n✑ Name: pong\n✑ Namespace: ing-internal\n✑ Exposing service hello on path /hello using service port 5678</p>\n<ol start=\"11\">\n<li>Task -</li>\n</ol>\n<p>Monitor the logs of pod foo and:\n✑ Extract log lines corresponding to error file-not-found\n✑ Write them to /opt/KUTR00101/foo</p>\n<ol start=\"12\">\n<li>Context -</li>\n</ol>\n<p>An existing Pod needs to be integrated into the Kubernetes built-in logging architecture (e.g. kubectl logs). Adding a streaming sidecar container is a good and common way to accomplish this requirement.</p>\n<p>Task -\nAdd a sidecar container named sidecar, using the busybox image, to the existing Pod big-corp-app. The new sidecar container has to run the following command:</p>\n<p><code>/bin/sh -c \"tail -n+1 -f /var/log/big-corp-app. log\"</code></p>\n<p>Use a Volume, mounted at /var/log, to make the log file big-corp-app.log available to the sidecar container.</p>\n<p><code>kubectl get pod big-corp-app -o yaml > big-corp-app.yaml</code></p>\n<ol start=\"13\">\n<li>Task -</li>\n</ol>\n<p>From the pod label name=overloaded-cpu, find pods running high CPU workloads and write the name of the pod consuming most CPU to the file /opt/\nKUTR00401/KUTR00401.txt (which already exists).</p>\n<ol start=\"14\">\n<li>Create a new PersistentVolumeClaim:</li>\n</ol>\n<p>✑ Name: pv-volume\n✑ Class: csi-hostpath-sc\n✑ Capacity: 10Mi\nCreate a new Pod which mounts the PersistentVolumeClaim as a volume:\n✑ Name: web-server\n✑ Image: nginx\n✑ Mount path: /usr/share/nginx/html\nConfigure the new Pod to have ReadWriteOnce access on the volume.\nFinally, using kubectl edit or kubectl patch expand the PersistentVolumeClaim to a capacity of 70Mi and record that change.</p>\n<ol start=\"15\">\n<li>Context -</li>\n</ol>\n<pre><code>You have been asked to create a new ClusterRole for a deployment pipeline and bind it to a specific ServiceAccount scoped to a specific namespace.\n\nTask -   \nCreate a new ClusterRole named deployment-clusterrole, which only allows to create the following resource types:\n✑ Deployment\n✑ Stateful Set\n✑ DaemonSet\n\nCreate a new ServiceAccount named cicd-token in the existing namespace app-team1.\n\nBind the new ClusterRole deployment-clusterrole to the new ServiceAccount cicd-token, limited to the namespace app-team1.\n</code></pre>\n<ol start=\"16\">\n<li>Task -</li>\n</ol>\n<p>A Kubernetes worker node, named wk8s-node-0 is in state NotReady.\nInvestigate why this is the case, and perform any appropriate steps to bring the node to a Ready state, ensuring that any changes are made permanent.</p>\n<ol start=\"17\">\n<li>Task -</li>\n</ol>\n<p>Given an existing Kubernetes cluster running version 1.22.1, upgrade all of the Kubernetes control plane and node components on the master node only to version 1.22.2.<br>\nBe sure to drain the master node before upgrading it and uncordon it after the upgrade.</p>\n<h1>복기</h1>\n<p>sidecar, ingress, systemctl이 특히나 어려웠다. 아마 이 문제들을 틀리지 않았을까 싶다.</p>"}},"pageContext":{"slug":"/kubernetes/CKA/cka-dday/"}},"staticQueryHashes":[],"slicesMap":{}}