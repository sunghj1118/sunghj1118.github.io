<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta name="generator" content="Gatsby 5.13.7"/><meta name="theme-color" content="#663399"/><style data-href="/styles.5250b878f8ec5a9f94b0.css" data-identity="gatsby-global-css">body{background-color:#f5f5f5;color:#333;font-family:Roboto,sans-serif;margin:0;padding:0}a{color:#007acc;text-decoration:none}a:hover{text-decoration:underline}h1,h2,h3,h4,h5,h6{font-family:Montserrat,sans-serif}button{font-family:Poppins,sans-serif}.markdown-body{background:#fff;border-radius:8px;box-shadow:0 2px 4px rgba(0,0,0,.1);font-family:Roboto,sans-serif;line-height:1.6;margin:2rem 0;padding:2rem}.markdown-body blockquote{border-left:.25em solid #dfe2e5;color:#6a737d;margin:0;padding:0 1em}.markdown-body pre{background-color:#f6f8fa;border-radius:8px;overflow:auto;padding:16px}.markdown-body code{background:rgba(27,31,35,.05);border-radius:3px;padding:.2em .4em}.markdown-body pre code{white-space:pre-wrap;word-break:break-word}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{font-weight:700;margin-bottom:1rem;margin-top:1.5rem}.markdown-body p{margin-bottom:1rem}.markdown-body a{word-wrap:break-word;overflow-wrap:break-word;word-break:break-word}</style><style data-styled="" data-styled-version="5.3.11">.fSgxj{max-width:700px;margin:0 auto;padding:0 20px;}/*!sc*/
data-styled.g1[id="Container-sc-16528bm-0"]{content:"fSgxj,"}/*!sc*/
.dolPew{background:#f0f0f0;padding:0.5rem;text-align:center;font-size:0.8rem;border-top:1px solid #ccc;font-family:'Lato',sans-serif;}/*!sc*/
data-styled.g2[id="Footer__FooterContainer-sc-z09s33-0"]{content:"dolPew,"}/*!sc*/
.ijVxnH{color:#007BFF;-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
.ijVxnH:hover{-webkit-text-decoration:underline;text-decoration:underline;}/*!sc*/
data-styled.g3[id="Footer__FooterLink-sc-z09s33-1"]{content:"ijVxnH,"}/*!sc*/
.eravZ{font-size:2.5rem;margin:0.5rem 0 0.5rem 0;}/*!sc*/
data-styled.g5[id="blog-post__PostTitle-sc-1iu9sol-0"]{content:"eravZ,"}/*!sc*/
.kXOzhE{font-size:1rem;color:#666;margin-bottom:1rem;}/*!sc*/
data-styled.g6[id="blog-post__PostDate-sc-1iu9sol-1"]{content:"kXOzhE,"}/*!sc*/
.irwwWE{display:inline-block;margin-bottom:0.5rem;padding:0.3rem 0.6rem;background-color:#6a9b86;color:white;-webkit-text-decoration:none;text-decoration:none;border-radius:3px;font-size:0.875rem;font-weight:normal;}/*!sc*/
.irwwWE:hover{background-color:#005fa3;}/*!sc*/
data-styled.g7[id="blog-post__AllPostsButton-sc-1iu9sol-2"]{content:"irwwWE,"}/*!sc*/
</style><link rel="icon" href="/favicon-32x32.png?v=b2dec66daa6289e2a332fc74ed123411" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=b2dec66daa6289e2a332fc74ed123411"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=b2dec66daa6289e2a332fc74ed123411"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=b2dec66daa6289e2a332fc74ed123411"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=b2dec66daa6289e2a332fc74ed123411"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=b2dec66daa6289e2a332fc74ed123411"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=b2dec66daa6289e2a332fc74ed123411"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=b2dec66daa6289e2a332fc74ed123411"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=b2dec66daa6289e2a332fc74ed123411"/><link href="https://fonts.googleapis.com/css?family=Roboto:400,700|Montserrat:400,700|Poppins:400,700&amp;display=swap" rel="stylesheet"/><script data-goatcounter="https://sunghj1118.goatcounter.com/count" async="" src="//gc.zgo.at/count.js"></script></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div class="Container-sc-16528bm-0 fSgxj"><div class="markdown-body"><a class="blog-post__AllPostsButton-sc-1iu9sol-2 irwwWE" href="/">&lt; All posts</a><h1 class="blog-post__PostTitle-sc-1iu9sol-0 eravZ">CKA 시험 준비 기출문제</h1><p class="blog-post__PostDate-sc-1iu9sol-1 kXOzhE">November 02, 2024</p><div><h1>CKA 기출문제 회독</h1>
<p>CKA는 기출문제가 많이 공개되고, 또 재출제율이 매우 높은 시험으로 알고 있다. 그래서 이번 기회에는 한번 기출문제들을 한번 씩 읽어보고 받아 쳐보고 공부해보려 한다.</p>
<h2>Exam Topics 기출문제</h2>
<p><a href="https://www.examtopics.com/exams/cncf/cka/view/">https://www.examtopics.com/exams/cncf/cka/view/</a></p>
<p><strong>Question #1</strong></p>
<pre><code>Context -  
You have been asked to create a new ClusterRole for a deployment pipeline and bind it to a specific ServiceAccount scoped to a specific namespace.

Task -   
Create a new ClusterRole named deployment-clusterrole, which only allows to create the following resource types:
✑ Deployment
✑ Stateful Set
✑ DaemonSet

Create a new ServiceAccount named cicd-token in the existing namespace app-team1.

Bind the new ClusterRole deployment-clusterrole to the new ServiceAccount cicd-token, limited to the namespace app-team1.
</code></pre>
<p><code>kubectl config use-context k8s</code></p>
<p><code>kubectl create clusterrole deployment-clusterrole --verb=create --resource=Deployment,StatefulSet,DaemonSet</code></p>
<p><code>kubectl create sa cicd-token -n app-team1</code></p>
<p><code>kubectl create clusterrolebinding deploy-b --clusterrole=deployment-clusterrole --serviceaccount=app-team1:cicd-token</code></p>
<p>(오늘 공교롭게도 스터디에서 나온 주제다. 클러스터롤, 클러스터 롤 바인딩, 그리고 SA)</p>
<p><strong>Question #2</strong><br>
Task -<br>
Set the node named ek8s-node-0 as unavailable and reschedule all the pods running on it.</p>
<p><code>kubectl config use-context ek8s</code></p>
<p><code>kubectl get nodes</code></p>
<p><code>kubectl drain ek8s-node-0 --ignore-daemonsets</code></p>
<ul>
<li>이때 사용되는 <code>drain</code> 명령어는 해당 노드를 비활성화하고, 해당 노드에 있는 모든 파드를 다른 노드로 이동시킨다. <code>--ignore-daemonsets</code> 옵션을 사용하면 데몬셋을 무시하고 파드를 이동시킨다.</li>
</ul>
<p><code>kubectl drain ek8s-node-0 --ignore-daemonsets --delete-emptydir-data</code></p>
<ul>
<li><code>--delete-emptydir-data</code> 옵션을 사용하면 노드를 비활성화하기 전에 해당 노드의 emptyDir 볼륨을 삭제한다.</li>
</ul>
<p><code>kubectl get nodes</code></p>
<p>(문제에서는 ek8s-node-0를 비활성화하라고 했지만, 사진에서는 왜 ek8s-node-1을 비활성화했는지 모르겠다.)</p>
<p><strong>Question #3</strong><br>
Task -<br>
Given an existing Kubernetes cluster running version 1.22.1, upgrade all of the Kubernetes control plane and node components on the master node only to version 1.22.2.<br>
Be sure to drain the master node before upgrading it and uncordon it after the upgrade.</p>
<p><code>kubectl config use-context mk8s</code></p>
<p><code>kubectl get nodes</code></p>
<p><code>kubectl drain mk8s-master-0 --ignore-daemonsets</code></p>
<ul>
<li>cordoned 뜻은 노드에 파드가 스케줄링 되지 않도록 하는 것이고, drain은 노드에 있는 파드를 다른 노드로 이동시키는 것이다.</li>
</ul>
<p><code>ssh mk8s-master-0</code></p>
<p><code>sudo -i</code></p>
<ul>
<li>이때 -i 옵션의 뜻은 root로 로그인하라는 뜻이다.</li>
</ul>
<p><code>apt install kubeadm=1.22.2-00 kubelet=1.22.2-00</code></p>
<p><code>kubeadm upgrade apply v1.22.2</code></p>
<p><code>systemctl restart kubelet</code></p>
<p><code>exit</code></p>
<ul>
<li>root에서 나가기</li>
</ul>
<p><code>exit</code></p>
<ul>
<li>ssh에서 나가기</li>
</ul>
<p><code>kubectl uncordon mk8s-master-0</code></p>
<ul>
<li>uncordon은 노드에 파드가 다시 스케줄링 되도록 하는 것이다.</li>
</ul>
<p><code>kubectl get nodes</code></p>
<ul>
<li>마스터 노드가 다시 정상적으로 작동하는지 확인한다.</li>
</ul>
<ol start="4">
<li>Task -</li>
</ol>
<p>First, create a snapshot of the existing etcd instance running at <a href="https://127.0.0.1:2379">https://127.0.0.1:2379</a>, saving the snapshot to /var/lib/backup/etcd-snapshot.db.</p>
<p>The following TLS certificates/key are supplied for connecting to the server with etcdctl :
• CA certificate: /opt/KUIN00601/ca.crt
• Client certificate:
/opt/KUIN00601/etcd-client.crt
• Client key:
/opt/KUIN00601/etcd-client.key</p>
<p>Creating a snapshot of the given instance is expected to complete in seconds.
If the operation seems to hang, something's likely wrong with your command. Use CTRL + c to cancel the operation and try again.</p>
<p>Next, restore an existing, previous snapshot located at /var/lib/backup/etcd-snapshot-previous.db.</p>
<pre><code class="language-bash">ETCDCTL_API=3 etcdctl snapshot save /var/lib/backup/etcd-snapshot.db \
--endpoints=https://127.0.0.1:2379 \
--cacert=/opt/KUIN00601/ca.crt \
--cert=/opt/KUIN00601/etcd-client.crt \
--key=/opt/KUIN00601/etcd-client.key
</code></pre>
<p><code>ETCDCTL_API=3 etcdctl snapshot restore /var/lib/backup/etcd-snapshot-previous.db --data-dir /var/lib/etcd-restored</code></p>
<ol start="5">
<li>Task -</li>
</ol>
<p>Create a new NetworkPolicy named allow-port-from-namespace in the existing namespace fubar.
Ensure that the new NetworkPolicy allows Pods in namespace internal to connect to port 9000 of Pods in namespace fubar.
Further ensure that the new NetworkPolicy:
✑ does not allow access to Pods, which don't listen on port 9000
✑ does not allow access from Pods, which are not in namespace internal</p>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-port-from-namespace
  namespace: fubar
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: internal
    ports:
    - protocol: TCP
      port: 9000
</code></pre>
<ol start="6">
<li>Task -</li>
</ol>
<p>Reconfigure the existing deployment front-end and add a port specification named http exposing port 80/tcp of the existing container nginx.
Create a new service named front-end-svc exposing the container port http.
Configure the new service to also expose the individual Pods via a NodePort on the nodes on which they are scheduled.</p>
<ul>
<li>add a port specification named http exposing port 80/tcp of the existing container nginx.</li>
</ul>
<pre><code class="language-yaml">spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
      name: http
</code></pre>
<ul>
<li>create a new service named front-end-svc exposing the container port http.</li>
</ul>
<pre><code class="language-bash">kubectl expose deployment front-end --name=front-end-svc --port=80 --target-port=http --type=NodePort
</code></pre>
<ol start="7">
<li>Task -</li>
</ol>
<p>Scale the deployment presentation to 3 pods.</p>
<p><code>kubectl scale deployment presentation --replicas=3</code></p>
<ol start="8">
<li>Task -</li>
</ol>
<p>Schedule a pod as follows:
✑ Name: nginx-kusc00401
✑ Image: nginx
✑ Node selector: disk=ssd</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: nginx-kusc00401
spec:
  nodeSelector:
    disk: ssd
  containers:
  - name: nginx
    image: nginx
</code></pre>
<ol start="9">
<li>Task -</li>
</ol>
<p>Check to see how many nodes are ready (not including nodes tainted NoSchedule) and write the number to /opt/KUSC00402/kusc00402.txt.</p>
<p><code>kubectl get nodes -o jsonpath='{.items[?(@.spec.taints[?(@.effect=="NoSchedule") == null])].status.conditions[?(@.type=="Ready")].status}' | grep True | wc -l > /opt/KUSC00402/kusc00402.txt</code></p>
<p>or easy way:</p>
<p><code>k get nodes</code><br>
<code>echo '2' > /opt/KUSC00402/kusc00402.txt</code></p>
<ol start="10">
<li>Task -</li>
</ol>
<p>Schedule a Pod as follows:
✑ Name: kucc8
✑ App Containers: 2
✑ Container Name/Images:</p>
<ul>
<li>nginx</li>
<li>consul</li>
</ul>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: kucc8
spec:
  containers:
  - name: nginx
    image: nginx
  - name: consul
    image: consul
</code></pre>
<p><code>kubectl apply -f kucc8-pod.yaml</code></p>
<ol start="11">
<li>Task -</li>
</ol>
<p>Create a persistent volume with name app-data, of capacity 2Gi and access mode ReadOnlyMany. The type of volume is hostPath and its location is /srv/app- data.</p>
<pre><code class="language-yaml">apiVersion: v1
kind: PersistentVolume
metadata:
  name: app-data
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadOnlyMany
  hostPath:
    path: /srv/app-data
</code></pre>
<ol start="12">
<li>Task -</li>
</ol>
<p>Monitor the logs of pod foo and:
✑ Extract log lines corresponding to error file-not-found
✑ Write them to /opt/KUTR00101/foo</p>
<p><code>kubectl logs foo | grep "file-not-found" > /opt/KUTR00101/foo</code></p>
<ol start="13">
<li>Context -</li>
</ol>
<p>An existing Pod needs to be integrated into the Kubernetes built-in logging architecture (e.g. kubectl logs). Adding a streaming sidecar container is a good and common way to accomplish this requirement.</p>
<p>Task -
Add a sidecar container named sidecar, using the busybox image, to the existing Pod big-corp-app. The new sidecar container has to run the following command:</p>
<p><code>/bin/sh -c "tail -n+1 -f /var/log/big-corp-app. log"</code></p>
<p>Use a Volume, mounted at /var/log, to make the log file big-corp-app.log available to the sidecar container.</p>
<p><code>kubectl get pod big-corp-app -o yaml > big-corp-app.yaml</code></p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: big-corp-app
spec:
  containers:
  - name: main-container
    image: busybox
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: sidecar
    image: busybox
    command: ["/bin/sh", "-c", "tail -n+1 -f /var/log/big-corp-app.log"]
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  volumes:
  - name: varlog
    emptyDir: {}
</code></pre>
<ol start="14">
<li>Task -</li>
</ol>
<p>From the pod label name=overloaded-cpu, find pods running high CPU workloads and write the name of the pod consuming most CPU to the file /opt/
KUTR00401/KUTR00401.txt (which already exists).</p>
<p><code>kubectl top pods -l name=overloaded-cpu --sort-by=cpu --no-headers | head -n 1 | awk '{print $1}' > /opt/KUTR00401/KUTR00401.txt</code></p>
<ol start="15">
<li>Task -</li>
</ol>
<p>A Kubernetes worker node, named wk8s-node-0 is in state NotReady.
Investigate why this is the case, and perform any appropriate steps to bring the node to a Ready state, ensuring that any changes are made permanent.</p>
<p><code>sudo systemctl restart kubelet</code><br>
<code>sudo systemctl restart kube-proxy</code></p>
<ol start="16">
<li>Task -</li>
</ol>
<p>Create a new PersistentVolumeClaim:
✑ Name: pv-volume
✑ Class: csi-hostpath-sc
✑ Capacity: 10Mi
Create a new Pod which mounts the PersistentVolumeClaim as a volume:
✑ Name: web-server
✑ Image: nginx
✑ Mount path: /usr/share/nginx/html
Configure the new Pod to have ReadWriteOnce access on the volume.
Finally, using kubectl edit or kubectl patch expand the PersistentVolumeClaim to a capacity of 70Mi and record that change.</p>
<ul>
<li>Create a new PersistentVolumeClaim:</li>
</ul>
<pre><code class="language-yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pv-volume
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Mi
  storageClassName: csi-hostpath-sc
</code></pre>
<ul>
<li>Create a new Pod which mounts the PersistentVolumeClaim as a volume:</li>
</ul>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: web-server
spec:
  containers:
    - name: nginx
      image: nginx
      volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: web-volume
  volumes:
    - name: web-volume
      persistentVolumeClaim:
        claimName: pv-volume
</code></pre>
<ul>
<li>Expand the PersistentVolumeClaim to a capacity of 70Mi:</li>
</ul>
<p><code>kubectl edit pvc pv-volume</code></p>
<ol start="17">
<li>Task -</li>
</ol>
<p>Create a new nginx Ingress resource as follows:
✑ Name: pong
✑ Namespace: ing-internal
✑ Exposing service hello on path /hello using service port 5678</p>
<ul>
<li>Create a new nginx Ingress resource:</li>
</ul>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: pong
  namespace: ing-internal
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /hello
        pathType: Prefix
        backend:
          service:
            name: hello
            port:
              number: 5678
</code></pre></div></div></div><footer class="Footer__FooterContainer-sc-z09s33-0 dolPew"><p>© <!-- -->2024<!-- --> HyunJoon Sung. All Rights Reserved.</p><p><a href="https://github.com/sunghj1118" class="Footer__FooterLink-sc-z09s33-1 ijVxnH">GitHub</a></p></footer></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/kubernetes/CKA/cka-dumps/";/*]]>*/</script><!-- slice-start id="_gatsby-scripts-1" -->
          <script
            id="gatsby-chunk-mapping"
          >
            window.___chunkMapping="{\"app\":[\"/app-2a748b101fc84366569c.js\"],\"component---src-pages-404-js\":[\"/component---src-pages-404-js-60d2a0d2c248384ad0d2.js\"],\"component---src-pages-index-js\":[\"/component---src-pages-index-js-593ef84ffd916dd56b25.js\"],\"component---src-pages-page-2-js\":[\"/component---src-pages-page-2-js-50e83426ef775361f1a0.js\"],\"component---src-pages-projects-js\":[\"/component---src-pages-projects-js-09d84881e2521f783e15.js\"],\"component---src-pages-using-ssr-js\":[\"/component---src-pages-using-ssr-js-f4952d7ab534af96aca0.js\"],\"component---src-pages-using-typescript-tsx\":[\"/component---src-pages-using-typescript-tsx-55e14b7949e0e0236a5b.js\"],\"component---src-templates-blog-post-js\":[\"/component---src-templates-blog-post-js-a5dcc9dbff974058c7d9.js\"],\"component---src-templates-tag-js\":[\"/component---src-templates-tag-js-84a93951c485ea92cc02.js\"]}";
          </script>
        <script>window.___webpackCompilationHash="885ecb13b1d4f4b10784";</script><script src="/webpack-runtime-a14314fed5ac8e7c9263.js" async></script><script src="/framework-a880249783d1bea64870.js" async></script><script src="/app-2a748b101fc84366569c.js" async></script><!-- slice-end id="_gatsby-scripts-1" --></body></html>